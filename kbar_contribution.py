# kbar_contribution.py
# -*- coding: utf-8 -*-
"""
kbar_contribution.py
--------------------
ç›®çš„ï¼šåšä½ è¦çš„ã€Œå¹´Kåˆ†ç®± + é€±/æœˆ/æ¼²åœå°å¹´Kçš„è²¢ç»åº¦ï¼ˆå«é›†ä¸­/ç·©æ¼²åˆ¤æ–·ï¼‰ã€çš„è³‡æ–™å±¤ï¼Œ
å¯«å› SQLite ä¾›å„€è¡¨æ¿ç›´æ¥æŸ¥ã€‚

âœ… ä¾è³´ï¼ˆå»ºè­°å…ˆè·‘å®Œï¼‰ï¼š
1) processor.py  -> stock_analysisï¼ˆå« is_limit_up, lu_type, consecutive_limits...ï¼‰
2) kbar_aggregator.py -> kbar_weekly / kbar_monthly / kbar_yearly
3) (å¯é¸) event_engine.py -> limit_up_events / daytrade_eventsï¼ˆè‹¥ä½ ä¹‹å¾Œè¦ç”¨äº‹ä»¶è¡¨åšæ›´ç´°çš„çµ±è¨ˆï¼‰

âœ… é€™æ”¯æœƒç”¢ç”Ÿï¼ˆå¯«å› DBï¼‰ï¼š
- year_contributionï¼šæ¯ä¸€æª”æ¯ä¸€å¹´ä¸€ç­†ï¼ˆæœ€é‡è¦ï¼‰
  åŒ…å«ï¼š
  - year_ret% / year_logret
  - å¹´Kåˆ†ç®±ï¼š100%é–“éš”(0~1000+)ã€0~100%å…§å†10%ç´°åˆ†ã€ä»¥åŠ0-50/50-100
  - é€±/æœˆé›†ä¸­åº¦ï¼štop1 week / top4 weeks / top1 month / top3 months çš„ log-return ä½”å¹´log-returnæ¯”ä¾‹
  - æ¼²åœè²¢ç»ï¼špeak_date å‰æ¼²åœæ ¹æ•¸ã€peak_date å‰æ¼²åœlogè²¢ç»å æ¯”ï¼ˆç”¨ log-return åŠ ç¸½ï¼‰
  - peak_date å‰ã€Œé€±/æœˆã€è²¢ç»ï¼ˆå¯ç”¨ä¾†å›ç­”ï¼šé£†è‚¡æ˜¯ä¸æ˜¯åœ¨ peak å‰å°±æ¼²å®Œï¼‰
- year_contribution_binsï¼šä¾å¹´Kåˆ†ç®±å½™ç¸½ï¼ˆå¹³å‡/ä¸­ä½æ•¸/æ¨£æœ¬æ•¸ï¼‰

ğŸ“Œ ä½ è¦ç ”ç©¶çš„ä¸»é¡Œï¼ˆé€™å¼µè¡¨ç›´æ¥æ”¯æ´ï¼‰ï¼š
- é£†è‚¡æ˜¯å¦ã€Œé›†ä¸­åœ¨ä¸€é€±/ä¸€å€‹æœˆå°±æ¼²å®Œã€ï¼š
  çœ‹ top1_week_share / top1_month_share æ˜¯å¦å¾ˆé«˜ï¼ˆä¾‹å¦‚ >0.4ã€>0.6ï¼‰
- æ¼²åœæ¿æ˜¯å¦æ˜¯ä¸»è¦è²¢ç»ä¾†æºï¼š
  çœ‹ limitup_log_share_to_peakã€limitup_count_to_peak
- é€±/æœˆå°å¹´Ké«˜é»çš„è²¢ç»ï¼š
  çœ‹ week_log_share_to_peak / month_log_share_to_peak

ç”¨æ³•ï¼š
    python kbar_contribution.py tw_stock_warehouse.db
æˆ–ï¼š
    from kbar_contribution import build_contribution_tables
    build_contribution_tables("tw_stock_warehouse.db")

"""

import sys
import sqlite3
import numpy as np
import pandas as pd
from typing import Optional, Dict

SQLITE_TIMEOUT = 120


# -----------------------------------------------------------------------------
# helpers
# -----------------------------------------------------------------------------
def _safe_logret(p0: float, p1: float) -> float:
    if p0 is None or p1 is None:
        return 0.0
    if not np.isfinite(p0) or not np.isfinite(p1):
        return 0.0
    if p0 <= 0 or p1 <= 0:
        return 0.0
    return float(np.log(p1 / p0))


def _bin_year_ret_100(ret_pct: float) -> str:
    """
    å¹´Kåˆ†ç®±ï¼š0~1000 æ¯100 + >1000
    æ³¨æ„ï¼šåªå°ã€Œæ­£å ±é…¬ã€åš 0~1000ï¼›è² å ±é…¬å¦å¤–ç¨ç«‹
    """
    if not np.isfinite(ret_pct):
        return "NA"
    if ret_pct < 0:
        return "NEGATIVE"
    if ret_pct >= 1000:
        return "1000UP"
    # 0~999.999
    lo = int(ret_pct // 100) * 100
    hi = lo + 100
    return f"{lo:04d}-{hi:04d}"


def _bin_year_ret_10_under100(ret_pct: float) -> str:
    """
    0~100% å…§å†ç´°åˆ† 10% ä¸€æ ¼ï¼Œå…¶é¤˜å›å‚³ OTHER
    """
    if not np.isfinite(ret_pct):
        return "NA"
    if ret_pct < 0:
        return "NEGATIVE"
    if ret_pct >= 100:
        return "GE_100"
    lo = int(ret_pct // 10) * 10
    hi = lo + 10
    return f"{lo:02d}-{hi:02d}"


def _bin_year_ret_50_under100(ret_pct: float) -> str:
    """
    0-50 / 50-100 / >=100 / negative
    """
    if not np.isfinite(ret_pct):
        return "NA"
    if ret_pct < 0:
        return "NEGATIVE"
    if ret_pct < 50:
        return "00-50"
    if ret_pct < 100:
        return "50-100"
    return "100UP"


def _topk_share(logrets: np.ndarray, denom: float, k: int) -> float:
    """
    top-k æ­£logret ä½”æ¯”ï¼ˆç”¨ log-return è¨ˆç®—ï¼Œé¿å…ç™¾åˆ†æ¯”åŠ ç¸½åèª¤ï¼‰
    denom <=0 æ™‚å› 0
    """
    if denom <= 0:
        return 0.0
    if logrets.size == 0:
        return 0.0
    pos = logrets[np.isfinite(logrets) & (logrets > 0)]
    if pos.size == 0:
        return 0.0
    pos_sorted = np.sort(pos)[::-1]
    return float(np.sum(pos_sorted[:k]) / denom)


def _sum_share(logrets: np.ndarray, denom: float) -> float:
    if denom <= 0:
        return 0.0
    if logrets.size == 0:
        return 0.0
    s = float(np.nansum(logrets[np.isfinite(logrets) & (logrets > 0)]))
    return float(s / denom)


# -----------------------------------------------------------------------------
# core
# -----------------------------------------------------------------------------
def build_contribution_tables(db_path: str, only_markets: Optional[set] = None) -> Dict[str, int]:
    """
    è®€å–ï¼š
      - kbar_yearly / kbar_monthly / kbar_weekly
      - stock_analysisï¼ˆç”¨ is_limit_up + daily logret åšæ¼²åœè²¢ç»ï¼‰
      - stock_pricesï¼ˆç”¨ä¾†æŠ“ peak_date çš„ closeï¼Œé¿å… peak_date è½åœ¨éäº¤æ˜“æ—¥/æˆ–ç¼ºæ¼ï¼‰
    ç”¢å‡ºï¼š
      - year_contribution
      - year_contribution_bins
    """
    conn = sqlite3.connect(db_path, timeout=SQLITE_TIMEOUT)

    try:
        # ---- æª¢æŸ¥å¿…è¦è¡¨ ----
        required = ["kbar_yearly", "kbar_monthly", "kbar_weekly", "stock_analysis", "stock_prices"]
        existing = set(pd.read_sql("SELECT name FROM sqlite_master WHERE type='table'", conn)["name"].tolist())
        missing = [t for t in required if t not in existing]
        if missing:
            raise RuntimeError(
                f"ç¼ºå°‘å¿…è¦è¡¨ï¼š{missing}\n"
                f"è«‹å…ˆè·‘ï¼šprocessor.pyï¼ˆstock_analysisï¼‰èˆ‡ kbar_aggregator.pyï¼ˆkbar_*ï¼‰"
            )

        # ---- è®€å–å¹´K ----
        y = pd.read_sql(
            """
            SELECT symbol, year, period_start, period_end,
                   open AS y_open, close AS y_close, high AS y_high,
                   year_peak_date, year_peak_high
            FROM kbar_yearly
            """,
            conn,
        )
        if y.empty:
            print("âŒ kbar_yearly ç„¡è³‡æ–™")
            return {"year_rows": 0, "bin_rows": 0}

        # è‹¥ä½ åªæƒ³åš tw/cn/jpï¼Œå¯å‚³ only_markets ä¸¦é  stock_info éæ¿¾
        if only_markets:
            info = pd.read_sql("SELECT symbol, market FROM stock_info", conn)
            y = y.merge(info, on="symbol", how="left")
            y = y[y["market"].str.lower().isin(set([m.lower() for m in only_markets]))].copy()
            y = y.drop(columns=["market"], errors="ignore")
            if y.empty:
                print("âŒ ä¾ only_markets éæ¿¾å¾Œ kbar_yearly ç„¡è³‡æ–™")
                return {"year_rows": 0, "bin_rows": 0}

        # æ—¥æœŸ
        y["period_start"] = pd.to_datetime(y["period_start"], errors="coerce")
        y["period_end"] = pd.to_datetime(y["period_end"], errors="coerce")
        y["year_peak_date"] = pd.to_datetime(y["year_peak_date"], errors="coerce")

        # å¹´å ±é…¬
        y["year_ret_pct"] = (y["y_close"].astype(float) / y["y_open"].astype(float) - 1.0) * 100.0
        y["year_logret"] = np.log(y["y_close"].astype(float) / y["y_open"].astype(float)).replace([np.inf, -np.inf], np.nan).fillna(0.0)

        # å¹´Kåˆ†ç®±
        y["year_ret_bin_100"] = y["year_ret_pct"].apply(_bin_year_ret_100)
        y["year_ret_bin_10_under100"] = y["year_ret_pct"].apply(_bin_year_ret_10_under100)
        y["year_ret_bin_50_under100"] = y["year_ret_pct"].apply(_bin_year_ret_50_under100)

        # ---- è®€ weekly/monthly ----
        w = pd.read_sql(
            """
            SELECT symbol, year, week_id, period_start, period_end, close AS w_close
            FROM kbar_weekly
            """,
            conn,
        )
        w["period_end"] = pd.to_datetime(w["period_end"], errors="coerce")
        w["year"] = w["year"].astype(int)

        m = pd.read_sql(
            """
            SELECT symbol, year, month_id, period_start, period_end, close AS m_close
            FROM kbar_monthly
            """,
            conn,
        )
        m["period_end"] = pd.to_datetime(m["period_end"], errors="coerce")
        m["year"] = m["year"].astype(int)

        # ---- stock_analysisï¼šç”¨ä¾†ç®—ã€Œæ¼²åœè²¢ç»ã€èˆ‡ peak å‰æ¼²åœæ ¹æ•¸ ----
        sa = pd.read_sql(
            """
            SELECT symbol, date, close, prev_close, is_limit_up
            FROM stock_analysis
            """,
            conn,
        )
        sa["date"] = pd.to_datetime(sa["date"], errors="coerce")
        sa = sa.dropna(subset=["date"]).sort_values(["symbol", "date"])
        sa["year"] = sa["date"].dt.year.astype(int)

        # daily logretï¼ˆç”¨ close/prev_closeï¼‰
        sa["d_logret"] = 0.0
        mask = (sa["close"].astype(float) > 0) & (sa["prev_close"].astype(float) > 0)
        sa.loc[mask, "d_logret"] = np.log(sa.loc[mask, "close"].astype(float) / sa.loc[mask, "prev_close"].astype(float))
        sa["is_limit_up"] = pd.to_numeric(sa["is_limit_up"], errors="coerce").fillna(0).astype(int)

        # ---- stock_pricesï¼šç‚ºäº†æŠ“ peak_date ç•¶å¤© closeï¼ˆè‹¥ peak_date ç¼ºæˆ–è½åœ¨ç¼ºå£ï¼‰ ----
        sp = pd.read_sql(
            """
            SELECT symbol, date, close
            FROM stock_prices
            """,
            conn,
        )
        sp["date"] = pd.to_datetime(sp["date"], errors="coerce")
        sp = sp.dropna(subset=["date"]).sort_values(["symbol", "date"])
        sp["year"] = sp["date"].dt.year.astype(int)

        # ---- per symbol-year è¨ˆç®— ----
        rows = []
        # ç”¨ merge keys è¿­ä»£å¹´è¡¨
        for _, r in y.iterrows():
            sym = r["symbol"]
            yr = int(r["year"])
            y_open = float(r["y_open"])
            y_close = float(r["y_close"])
            peak_date = r["year_peak_date"]

            # å¹´log denom
            denom = float(r["year_logret"])
            denom_pos = denom if denom > 0 else 0.0

            # --- weekly logrets within year ---
            wg = w[(w["symbol"] == sym) & (w["year"] == yr)].sort_values("period_end")
            w_close = wg["w_close"].astype(float).values
            w_log = np.array([])
            if w_close.size >= 2:
                w_log = np.log(w_close[1:] / w_close[:-1])
                w_log = w_log[np.isfinite(w_log)]
            top1_week_share = _topk_share(w_log, denom_pos, 1)
            top4_weeks_share = _topk_share(w_log, denom_pos, 4)
            sum_pos_week_share = _sum_share(w_log, denom_pos)

            # --- monthly logrets within year ---
            mg = m[(m["symbol"] == sym) & (m["year"] == yr)].sort_values("period_end")
            m_close = mg["m_close"].astype(float).values
            m_log = np.array([])
            if m_close.size >= 2:
                m_log = np.log(m_close[1:] / m_close[:-1])
                m_log = m_log[np.isfinite(m_log)]
            top1_month_share = _topk_share(m_log, denom_pos, 1)
            top3_months_share = _topk_share(m_log, denom_pos, 3)
            sum_pos_month_share = _sum_share(m_log, denom_pos)

            # --- peak_date å°é½Šï¼šæ‰¾ <= peak_date æœ€è¿‘äº¤æ˜“æ—¥ close ---
            peak_close = np.nan
            if pd.notna(peak_date):
                spt = sp[(sp["symbol"] == sym) & (sp["year"] == yr) & (sp["date"] <= peak_date)]
                if not spt.empty:
                    peak_close = float(spt.iloc[-1]["close"])
            # peak_logretï¼ˆyear_open â†’ peak_closeï¼‰
            peak_logret = _safe_logret(y_open, peak_close) if np.isfinite(peak_close) else 0.0

            # --- peak å‰ week/month log share ---
            week_log_to_peak = 0.0
            if denom_pos > 0 and pd.notna(peak_date) and not wg.empty:
                # å– period_end <= peak_date çš„é€±ï¼Œç®—ã€Œå¹´åˆâ†’è©²é€±æ”¶ç›¤ã€logret
                # ç”¨é€±æ”¶ç›¤åºåˆ—è¿‘ä¼¼ï¼šlog( last_w_close / first_w_close ) å†åŠ ä¸Šå¹´åˆ->ç¬¬ä¸€é€± close çš„èª¤å·®
                # æ›´ç²¾æº–å¯æ”¹ç”¨æ—¥Kï¼Œä½†é€™è£¡å…ˆç”¨å¯è·‘ç‰ˆæœ¬
                w_end = wg[wg["period_end"] <= peak_date]
                if len(w_end) >= 1:
                    w_last = float(w_end.iloc[-1]["w_close"])
                    week_log_to_peak = _safe_logret(y_open, w_last)
            week_log_share_to_peak = float(week_log_to_peak / denom_pos) if denom_pos > 0 else 0.0

            month_log_to_peak = 0.0
            if denom_pos > 0 and pd.notna(peak_date) and not mg.empty:
                m_end = mg[mg["period_end"] <= peak_date]
                if len(m_end) >= 1:
                    m_last = float(m_end.iloc[-1]["m_close"])
                    month_log_to_peak = _safe_logret(y_open, m_last)
            month_log_share_to_peak = float(month_log_to_peak / denom_pos) if denom_pos > 0 else 0.0

            # --- limit up count/log contribution to peak ---
            lug = sa[(sa["symbol"] == sym) & (sa["year"] == yr)]
            if pd.notna(peak_date):
                lug_to_peak = lug[lug["date"] <= peak_date]
            else:
                lug_to_peak = lug

            limitup_count_to_peak = int((lug_to_peak["is_limit_up"] == 1).sum())
            limitup_log_sum_to_peak = float(lug_to_peak.loc[lug_to_peak["is_limit_up"] == 1, "d_logret"].sum())
            limitup_log_share_to_peak = float(limitup_log_sum_to_peak / denom_pos) if denom_pos > 0 else 0.0

            rows.append(
                {
                    "symbol": sym,
                    "year": yr,

                    # å¹´K
                    "y_open": y_open,
                    "y_close": y_close,
                    "year_ret_pct": float(r["year_ret_pct"]),
                    "year_logret": denom,

                    # å¹´åˆ†ç®±
                    "year_ret_bin_100": r["year_ret_bin_100"],
                    "year_ret_bin_10_under100": r["year_ret_bin_10_under100"],
                    "year_ret_bin_50_under100": r["year_ret_bin_50_under100"],

                    # peak
                    "year_peak_date": peak_date.strftime("%Y-%m-%d") if pd.notna(peak_date) else None,
                    "year_peak_high": float(r["year_peak_high"]) if np.isfinite(r["year_peak_high"]) else np.nan,
                    "peak_close_aligned": peak_close if np.isfinite(peak_close) else np.nan,
                    "peak_logret_from_open": peak_logret,

                    # é€±/æœˆé›†ä¸­åº¦ï¼ˆå›ç­”ï¼šé›†ä¸­ä¸€é€±/ä¸€æœˆ vs ç·©æ¼²ï¼‰
                    "top1_week_share": top1_week_share,
                    "top4_weeks_share": top4_weeks_share,
                    "sum_pos_week_share": sum_pos_week_share,

                    "top1_month_share": top1_month_share,
                    "top3_months_share": top3_months_share,
                    "sum_pos_month_share": sum_pos_month_share,

                    # peak å‰é€±/æœˆå·²å®Œæˆåº¦ï¼ˆå›ç­”ï¼šæ˜¯å¦ peak å‰å°±æ¼²å®Œï¼‰
                    "week_log_share_to_peak": week_log_share_to_peak,
                    "month_log_share_to_peak": month_log_share_to_peak,

                    # æ¼²åœè²¢ç»ï¼ˆå›ç­”ï¼šé£†è‚¡æ˜¯å¦é æ¼²åœå †å‡ºä¾†ï¼‰
                    "limitup_count_to_peak": limitup_count_to_peak,
                    "limitup_log_sum_to_peak": limitup_log_sum_to_peak,
                    "limitup_log_share_to_peak": limitup_log_share_to_peak,
                }
            )

        out = pd.DataFrame(rows)
        if out.empty:
            print("âŒ year_contribution ç„¡è³‡æ–™ï¼ˆå¯èƒ½ year/kbar å°ä¸èµ·ä¾†ï¼‰")
            return {"year_rows": 0, "bin_rows": 0}

        # ---- å¯«å› year_contribution ----
        conn.execute("DROP TABLE IF EXISTS year_contribution")
        out.to_sql("year_contribution", conn, if_exists="replace", index=False)

        try:
            conn.execute("CREATE INDEX IF NOT EXISTS idx_year_contrib_symbol_year ON year_contribution(symbol, year)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_year_contrib_bin100 ON year_contribution(year_ret_bin_100)")
        except Exception:
            pass

        # ---- bins summaryï¼ˆä¾ 100% åˆ†ç®±ç‚ºä¸»ï¼›ä½ ä¹Ÿå¯ä»¥å¦å¤–åš under100 çš„ç´°åˆ†å½™ç¸½ï¼‰ ----
        def _agg_summary(df: pd.DataFrame) -> pd.Series:
            return pd.Series(
                {
                    "n": int(len(df)),
                    "avg_year_ret_pct": float(df["year_ret_pct"].mean()),
                    "median_year_ret_pct": float(df["year_ret_pct"].median()),

                    "avg_top1_week_share": float(df["top1_week_share"].mean()),
                    "avg_top1_month_share": float(df["top1_month_share"].mean()),
                    "avg_limitup_count_to_peak": float(df["limitup_count_to_peak"].mean()),
                    "avg_limitup_log_share_to_peak": float(df["limitup_log_share_to_peak"].mean()),

                    # â€œé›†ä¸­åº¦é–€æª»â€ ä½ å„€è¡¨æ¿å¸¸æœƒæƒ³çœ‹çš„æ¯”ä¾‹
                    "pct_top1_week_share_ge_0_4": float((df["top1_week_share"] >= 0.4).mean() * 100),
                    "pct_top1_month_share_ge_0_4": float((df["top1_month_share"] >= 0.4).mean() * 100),
                    "pct_limitup_log_share_ge_0_4": float((df["limitup_log_share_to_peak"] >= 0.4).mean() * 100),
                }
            )

        bins = out.groupby("year_ret_bin_100", sort=False).apply(_agg_summary).reset_index()
        conn.execute("DROP TABLE IF EXISTS year_contribution_bins")
        bins.to_sql("year_contribution_bins", conn, if_exists="replace", index=False)

        try:
            conn.execute("CREATE INDEX IF NOT EXISTS idx_year_contrib_bins_bin ON year_contribution_bins(year_ret_bin_100)")
        except Exception:
            pass

        conn.commit()

        print("\nâœ… kbar_contribution å®Œæˆï¼š")
        print(f"ğŸ“Œ year_contribution rows: {len(out):,}")
        print(f"ğŸ“Œ year_contribution_bins rows: {len(bins):,}")
        print("ğŸ“Œ ä½ å¯ä»¥ç›´æ¥ç”¨ year_contribution.top1_week_share / top1_month_share åˆ¤æ–·ã€é›†ä¸­ vs ç·©æ¼²ã€")
        print("ğŸ“Œ ç”¨ limitup_* æ¬„ä½åˆ¤æ–·ã€æ¼²åœæ˜¯å¦ä¸»è²¢ç»ã€")

        return {"year_rows": int(len(out)), "bin_rows": int(len(bins))}

    finally:
        conn.close()


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python kbar_contribution.py <db_path>")
        sys.exit(1)

    db = sys.argv[1]
    build_contribution_tables(db_path=db)
