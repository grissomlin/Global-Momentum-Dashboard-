# pages/dbcheck.py
# -*- coding: utf-8 -*-
"""
DB Check / Schema Explorer (Streamlit Page)
-------------------------------------------
ç›®çš„ï¼š
- æª¢æŸ¥ SQLite DB æ˜¯å¦å¯è®€
- åˆ—å‡ºæ¯å€‹ table çš„æ¬„ä½ (schema) èˆ‡å‰ 10 ç­†è³‡æ–™
- åœ¨ä¸‹æ–¹æä¾›ã€Œæ¬„ä½ä¸­è‹±æ–‡è§£é‡‹ï¼ˆData Dictionaryï¼‰ã€èˆ‡å¯ç›´æ¥è¤‡è£½è²¼ä¸Šå• AI çš„ prompt

âœ… ä¸ä¾è³´ data_cleaning.pyï¼ˆç´” DB è®€å–æª¢æŸ¥ï¼‰
"""

from __future__ import annotations

import os
import sqlite3
from typing import Dict, Optional, Tuple, List

import pandas as pd
import streamlit as st


# =========================
# Helpers
# =========================
@st.cache_data(show_spinner=False)
def list_tables(db_path: str) -> List[str]:
    conn = sqlite3.connect(db_path)
    try:
        df = pd.read_sql(
            "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%' ORDER BY name;",
            conn,
        )
        return df["name"].tolist()
    finally:
        conn.close()


@st.cache_data(show_spinner=False)
def read_table_schema(db_path: str, table: str) -> pd.DataFrame:
    conn = sqlite3.connect(db_path)
    try:
        # PRAGMA table_info returns: cid, name, type, notnull, dflt_value, pk
        rows = conn.execute(f"PRAGMA table_info('{table}')").fetchall()
        return pd.DataFrame(rows, columns=["cid", "name", "type", "notnull", "dflt_value", "pk"])
    finally:
        conn.close()


@st.cache_data(show_spinner=False)
def read_table_head(db_path: str, table: str, n: int = 10) -> pd.DataFrame:
    conn = sqlite3.connect(db_path)
    try:
        return pd.read_sql(f"SELECT * FROM '{table}' LIMIT {int(n)};", conn)
    finally:
        conn.close()


def _safe_read_scalar(conn: sqlite3.Connection, sql: str) -> Optional[float]:
    try:
        r = conn.execute(sql).fetchone()
        return r[0] if r else None
    except Exception:
        return None


@st.cache_data(show_spinner=False)
def get_quick_stats(db_path: str) -> Dict[str, Optional[float]]:
    """ç›¡é‡ä¸å‡è¨­ table ä¸€å®šå­˜åœ¨ï¼›æœ‰å°±é¡¯ç¤ºï¼Œæ²’æœ‰å°±ç•¥éã€‚"""
    conn = sqlite3.connect(db_path)
    try:
        stats: Dict[str, Optional[float]] = {}
        stats["tables"] = _safe_read_scalar(conn, "SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';")
        stats["stock_prices_rows"] = _safe_read_scalar(conn, "SELECT COUNT(*) FROM stock_prices;")
        stats["stock_analysis_rows"] = _safe_read_scalar(conn, "SELECT COUNT(*) FROM stock_analysis;")
        stats["limit_up_events_rows"] = _safe_read_scalar(conn, "SELECT COUNT(*) FROM limit_up_events;")
        stats["daytrade_events_rows"] = _safe_read_scalar(conn, "SELECT COUNT(*) FROM daytrade_events;")
        stats["year_contribution_rows"] = _safe_read_scalar(conn, "SELECT COUNT(*) FROM year_contribution;")
        return stats
    finally:
        conn.close()


def _build_default_dictionary() -> Dict[str, Dict[str, Tuple[str, str]]]:
    """
    data_dictionary[table][column] = (ä¸­æ–‡, English)
    åªæ”¾ä½  repo å¸¸ç”¨è¡¨ï¼›é‡åˆ°æœªçŸ¥æ¬„ä½æœƒé¡¯ç¤ºç©ºç™½ï¼Œæ–¹ä¾¿ä½ è£œã€‚
    """
    dd: Dict[str, Dict[str, Tuple[str, str]]] = {}

    # -------------------------
    # stock_info
    # -------------------------
    dd["stock_info"] = {
        "symbol": ("è‚¡ç¥¨ä»£è™Ÿï¼ˆå«å¸‚å ´å¾Œç¶´ï¼‰", "Ticker/Symbol (with market suffix)"),
        "name": ("å…¬å¸åç¨±", "Company name"),
        "market": ("å¸‚å ´ï¼ˆTW/CN/JP/HK/US...ï¼‰", "Market code (TW/CN/JP/HK/US...)"),
        "market_detail": ("å¸‚å ´ç´°åˆ†ï¼ˆä¸Šå¸‚/ä¸Šæ«ƒ/ç§‘å‰µæ¿ç­‰ï¼‰", "Market detail (TSE/OTC/STAR/...)"),
        "sector": ("ç”¢æ¥­åˆ¥", "Sector"),
        "industry": ("ç´°ç”¢æ¥­åˆ¥", "Industry"),
    }

    # -------------------------
    # stock_prices (raw or lightly cleaned)
    # -------------------------
    dd["stock_prices"] = {
        "symbol": ("è‚¡ç¥¨ä»£è™Ÿ", "Symbol"),
        "date": ("äº¤æ˜“æ—¥æœŸ", "Trading date"),
        "open": ("é–‹ç›¤åƒ¹", "Open"),
        "high": ("æœ€é«˜åƒ¹", "High"),
        "low": ("æœ€ä½åƒ¹", "Low"),
        "close": ("æ”¶ç›¤åƒ¹", "Close"),
        "adj_close": ("é‚„åŸæ”¶ç›¤åƒ¹ï¼ˆè‹¥æœ‰ï¼‰", "Adjusted close (if available)"),
        "volume": ("æˆäº¤é‡", "Volume"),
    }

    # -------------------------
    # stock_analysis (processor ç”¢ç‰©)
    # -------------------------
    dd["stock_analysis"] = {
        "symbol": ("è‚¡ç¥¨ä»£è™Ÿ", "Symbol"),
        "date": ("äº¤æ˜“æ—¥æœŸ", "Trading date"),
        "open": ("é–‹ç›¤åƒ¹", "Open"),
        "high": ("æœ€é«˜åƒ¹", "High"),
        "low": ("æœ€ä½åƒ¹", "Low"),
        "close": ("æ”¶ç›¤åƒ¹", "Close"),
        "volume": ("æˆäº¤é‡", "Volume"),
        "prev_close": ("å‰ä¸€æ—¥æ”¶ç›¤åƒ¹", "Previous close"),
        "daily_change": ("æ—¥å ±é…¬ç‡ï¼ˆclose/prev_close-1ï¼‰", "Daily return (close/prev_close - 1)"),
        "market": ("å¸‚å ´ä»£ç¢¼", "Market code"),
        "market_detail": ("å¸‚å ´ç´°åˆ†", "Market detail"),
        "sector": ("ç”¢æ¥­åˆ¥", "Sector"),
        "is_limit_up": ("æ˜¯å¦æ”¶æ¼²åœ", "Is limit-up at close"),
        "lu_type": ("æ¼²åœå‹æ…‹ï¼ˆä½ æ–‡ç« è¦å‰‡ï¼‰", "Limit-up pattern/type (your article rules)"),
        "consecutive_limits": ("é€£æ¿å¤©æ•¸", "Consecutive limit-up days"),
        "is_one_tick_lock": ("æ˜¯å¦ä¸€å­—é–", "One-tick lock (open=high=low=close)"),
        "limit_up_price": ("æ¼²åœåƒ¹ï¼ˆç²¾æº–ï¼‰", "Limit-up price (exact)"),
        "hit_limit_up_intraday": ("ç›¤ä¸­æ˜¯å¦è§¸åŠæ¼²åœ", "Hit limit-up intraday"),
        "limit_up_fail": ("ç›¤ä¸­åˆ°æ¼²åœä½†æ”¶ä¸ä½", "Hit limit-up but failed at close"),
    }

    # -------------------------
    # kbar_*
    # -------------------------
    dd["kbar_weekly"] = {
        "symbol": ("è‚¡ç¥¨ä»£è™Ÿ", "Symbol"),
        "year": ("å¹´ä»½ï¼ˆç”¨æ–¼åˆ†ç¾¤ï¼‰", "Year (grouping key)"),
        "week_id": ("é€±åºè™Ÿï¼ˆå¹´å…§ç¬¬å¹¾é€±ï¼‰", "Week id (within year)"),
        "period_start": ("é€±æœŸèµ·æ—¥ï¼ˆå«ï¼‰", "Period start (inclusive)"),
        "period_end": ("é€±æœŸè¨–æ—¥ï¼ˆå«ï¼‰", "Period end (inclusive)"),
        "open": ("é€±é–‹ç›¤åƒ¹", "Weekly open"),
        "high": ("é€±æœ€é«˜åƒ¹", "Weekly high"),
        "low": ("é€±æœ€ä½åƒ¹", "Weekly low"),
        "close": ("é€±æ”¶ç›¤åƒ¹", "Weekly close"),
        "volume": ("é€±æˆäº¤é‡", "Weekly volume"),
    }
    dd["kbar_monthly"] = {
        "symbol": ("è‚¡ç¥¨ä»£è™Ÿ", "Symbol"),
        "year": ("å¹´ä»½ï¼ˆç”¨æ–¼åˆ†ç¾¤ï¼‰", "Year (grouping key)"),
        "month_id": ("æœˆä»½ï¼ˆ1-12ï¼‰", "Month id (1-12)"),
        "period_start": ("é€±æœŸèµ·æ—¥ï¼ˆå«ï¼‰", "Period start (inclusive)"),
        "period_end": ("é€±æœŸè¨–æ—¥ï¼ˆå«ï¼‰", "Period end (inclusive)"),
        "open": ("æœˆé–‹ç›¤åƒ¹", "Monthly open"),
        "high": ("æœˆæœ€é«˜åƒ¹", "Monthly high"),
        "low": ("æœˆæœ€ä½åƒ¹", "Monthly low"),
        "close": ("æœˆæ”¶ç›¤åƒ¹", "Monthly close"),
        "volume": ("æœˆæˆäº¤é‡", "Monthly volume"),
    }
    dd["kbar_yearly"] = {
        "symbol": ("è‚¡ç¥¨ä»£è™Ÿ", "Symbol"),
        "year": ("å¹´ä»½", "Year"),
        "period_start": ("å¹´åº¦èµ·æ—¥ï¼ˆå«ï¼‰", "Period start (inclusive)"),
        "period_end": ("å¹´åº¦è¨–æ—¥ï¼ˆå«ï¼‰", "Period end (inclusive)"),
        "open": ("å¹´é–‹ç›¤åƒ¹", "Yearly open"),
        "high": ("å¹´æœ€é«˜åƒ¹", "Yearly high"),
        "low": ("å¹´æœ€ä½åƒ¹", "Yearly low"),
        "close": ("å¹´æ”¶ç›¤åƒ¹", "Yearly close"),
        "volume": ("å¹´æˆäº¤é‡", "Yearly volume"),
        "year_peak_date": ("å¹´åº¦æœ€é«˜é»æ—¥æœŸï¼ˆrawï¼‰", "Peak date within year (raw)"),
        "year_peak_high": ("å¹´åº¦æœ€é«˜åƒ¹", "Peak high within year"),
    }

    # -------------------------
    # year_contribution / eventsï¼ˆä½ çš„ç ”ç©¶è¡¨ï¼‰
    # -------------------------
    dd["year_contribution"] = {
        "symbol": ("è‚¡ç¥¨ä»£è™Ÿ", "Symbol"),
        "year": ("å¹´ä»½", "Year"),
        "year_ret_pct": ("å¹´åº¦å ±é…¬ç‡ï¼ˆ%ï¼‰", "Year return (%)"),
        "year_logret": ("å¹´åº¦ log return", "Year log return"),
        "burst_style_week": ("çˆ†ç™¼å‹æ…‹ï¼ˆé€±ï¼‰", "Burst style (week)"),
        "burst_style_month": ("çˆ†ç™¼å‹æ…‹ï¼ˆæœˆï¼‰", "Burst style (month)"),
        "year_peak_trade_date": ("å³°å€¼å°é½Šåˆ°å¯¦éš›äº¤æ˜“æ—¥", "Peak date aligned to trading day"),
        "share_year_to_peak": ("åˆ° peak çš„ logret ä½”å…¨å¹´æ­£å ±é…¬æ¯”ä¾‹", "Share of year positive logret achieved by peak"),
        "limitup_count_to_peak": ("peak å‰æ¼²åœå¤©æ•¸", "Limit-up count before peak"),
    }

    dd["limit_up_events"] = {
        "symbol": ("è‚¡ç¥¨ä»£è™Ÿ", "Symbol"),
        "date": ("äº‹ä»¶æ—¥æœŸ", "Event date"),
        "market": ("å¸‚å ´", "Market"),
        "sector": ("ç”¢æ¥­", "Sector"),
        "is_limit_up": ("æ”¶æ¼²åœ", "Limit-up at close"),
        "lu_type": ("æ¼²åœå‹æ…‹", "Limit-up pattern/type"),
        "consecutive_limits": ("é€£æ¿å¤©æ•¸", "Consecutive limit-ups"),
        "is_one_tick_lock": ("ä¸€å­—é–", "One-tick lock"),
        "next_open_ret": ("éš”æ—¥é–‹ç›¤å ±é…¬ï¼ˆnext_open/close-1ï¼‰", "Next-day open return (next_open/close - 1)"),
        "next_intraday_drawdown": ("éš”æ—¥ç›¤ä¸­å›æ’¤ï¼ˆnext_low/next_open-1ï¼‰", "Next-day intraday drawdown (next_low/next_open - 1)"),
        "ret_1d": ("æœªä¾† 1 æ—¥å ±é…¬ï¼ˆclose-basedï¼‰", "Forward 1D return (close-based)"),
        "ret_5d": ("æœªä¾† 5 æ—¥å ±é…¬ï¼ˆclose-basedï¼‰", "Forward 5D return (close-based)"),
        "max_up_5d": ("æœªä¾† 5 æ—¥æœ€å¤§ä¸Šæ¼²ï¼ˆhigh/close-1ï¼‰", "Max up in next 5D (high/close - 1)"),
        "max_dd_5d": ("æœªä¾† 5 æ—¥æœ€å¤§å›æ’¤ï¼ˆlow/close-1ï¼‰", "Max drawdown in next 5D (low/close - 1)"),
    }

    dd["daytrade_events"] = {
        "symbol": ("è‚¡ç¥¨ä»£è™Ÿ", "Symbol"),
        "date": ("äº‹ä»¶æ—¥æœŸ", "Event date"),
        "prev_limit_up_today_not": ("æ˜¨å¤©æ¼²åœã€ä»Šå¤©æ²’æ¼²åœ", "Prev day limit-up, today not"),
        "prev_limit_up_today_fail": ("æ˜¨å¤©æ¼²åœã€ä»Šå¤©è¡æ¼²åœå¤±æ•—", "Prev day limit-up, today fail"),
        "today_limit_up_fail_no_prev": ("æ˜¨å¤©æ²’æ¼²åœã€ä»Šå¤©è¡æ¼²åœå¤±æ•—", "No prev limit-up, today fail"),
        "today_limit_up_yes_no_prev": ("æ˜¨å¤©æ²’æ¼²åœã€ä»Šå¤©æ”¶æ¼²åœï¼ˆé¦–æ¿ï¼‰", "No prev limit-up, today limit-up"),
    }

    return dd


def render_data_dictionary(table: str, schema_df: pd.DataFrame, dd_map: Dict[str, Dict[str, Tuple[str, str]]]) -> pd.DataFrame:
    cols = schema_df["name"].tolist()
    tmap = dd_map.get(table, {})
    out_rows = []
    for c in cols:
        zh, en = ("", "")
        if c in tmap:
            zh, en = tmap[c]
        out_rows.append({"column": c, "ä¸­æ–‡èªªæ˜": zh, "English": en})
    return pd.DataFrame(out_rows)


def build_ai_prompt(db_path: str, table: str, schema_df: pd.DataFrame, head_df: pd.DataFrame, dict_df: pd.DataFrame) -> str:
    # ç›¡é‡çŸ­ä¸”å¥½è¤‡è£½ï¼šschema + sample + dictionary
    schema_lines = []
    for _, r in schema_df.iterrows():
        schema_lines.append(f"- {r['name']} ({r['type']}){' [PK]' if int(r['pk'])==1 else ''}")
    schema_txt = "\n".join(schema_lines)

    # sampleï¼šé¿å…å¤ªé•·
    sample_txt = head_df.to_csv(index=False)

    # dictionaryï¼šåªåˆ—å‡ºæœ‰å¡«èªªæ˜è€…ï¼ˆé¿å…æ·¹æ²’ï¼‰
    dict_filled = dict_df.copy()
    dict_filled["has_desc"] = (dict_filled["ä¸­æ–‡èªªæ˜"].astype(str).str.len() > 0) | (dict_filled["English"].astype(str).str.len() > 0)
    dict_filled = dict_filled[dict_filled["has_desc"]].drop(columns=["has_desc"], errors="ignore")

    dict_lines = []
    for _, r in dict_filled.iterrows():
        dict_lines.append(f"- {r['column']}: {r['ä¸­æ–‡èªªæ˜']} | {r['English']}")
    dict_txt = "\n".join(dict_lines) if dict_lines else "(No dictionary entries yet. Fill them in this page.)"

    return f"""ä½ æ˜¯è³‡æ·±é‡åŒ–/è³‡æ–™å·¥ç¨‹é¡§å•ã€‚è«‹å¹«æˆ‘æª¢æŸ¥é€™å€‹ SQLite è³‡æ–™è¡¨çš„è¨­è¨ˆæ˜¯å¦åˆç†ï¼Œä¸¦æŒ‡å‡ºï¼š
1) æ¬„ä½å‘½åæ˜¯å¦ä¸€è‡´ã€ç¼ºå“ªäº›å¿…è¦æ¬„ä½ã€æœ‰å“ªäº›å¯ç–‘æ¬„ä½æˆ–å‹åˆ¥
2) ä¾æˆ‘æä¾›çš„ sample å‰ 10 ç­†ï¼Œæ¨æ¸¬æ˜¯å¦æœ‰è³‡æ–™ç•°å¸¸æˆ–æ¬„ä½å®šç¾©ä¸æ¸…
3) çµ¦æˆ‘ 3-5 å€‹å»ºè­°ï¼šå¦‚ä½•è®“é€™å¼µè¡¨æ›´é©åˆç ”ç©¶/å›æ¸¬/å„€è¡¨æ¿æŸ¥è©¢

DB Path: {db_path}
Table: {table}

[Schema]
{schema_txt}

[Sample head (CSV, first 10 rows)]
{sample_txt}

[Data Dictionary]
{dict_txt}
"""


# =========================
# UI
# =========================
st.set_page_config(page_title="DB Check", layout="wide")

st.title("ğŸ§ª DB Checkï¼ˆè³‡æ–™åº«è®€å– + Schema + Sample + æ¬„ä½å­—å…¸ï¼‰")
st.caption("ç”¨é€”ï¼šç¢ºèª SQLite DB å¯æ­£å¸¸è®€å–ã€å¿«é€Ÿçœ‹åˆ°æ¯å€‹ table çš„æ¬„ä½èˆ‡å‰ 10 ç­†ï¼Œä¸¦ç”Ÿæˆå¯ç›´æ¥è²¼å»å• AI çš„ promptã€‚")

# è®“å®ƒè·Ÿ dashboard.py ä¸€æ¨£çš„ market_code å‘½åï¼ˆæœ‰ session_state å°±æ²¿ç”¨ï¼‰
MARKET_MAP = {
    "å°ç£ (TW)": "tw-share",
    "é¦™æ¸¯ (HK)": "hk-share",
    "ç¾åœ‹ (US)": "us-share",
    "æ—¥æœ¬ (JP)": "jp-share",
    "éŸ“åœ‹ (KR)": "kr-share",
    "ä¸­åœ‹ (CN)": "cn-share",
}

c1, c2, c3 = st.columns([1.2, 1.3, 1.5], vertical_alignment="bottom")

with c1:
    default_market_label = None
    if "market_selection" in st.session_state:
        # dashboard.py å¸¸ç”¨ keyï¼šmarket_selection = "å°ç£ (TW)"...
        default_market_label = st.session_state.get("market_selection")
    market_label = st.selectbox("Market", list(MARKET_MAP.keys()), index=list(MARKET_MAP.keys()).index(default_market_label) if default_market_label in MARKET_MAP else 0)

market_code = MARKET_MAP[market_label]
default_db = f"{market_code}_stock_warehouse.db"

with c2:
    db_path = st.text_input("SQLite DB è·¯å¾‘", value=st.session_state.get("db_path", default_db))

with c3:
    head_n = st.number_input("æ¯è¡¨é¡¯ç¤ºå‰ N ç­†", min_value=5, max_value=200, value=10, step=5)

# ä½ å¯èƒ½å¾ dashboard åŒæ­¥ DBï¼šåœ¨é€™é ä¹Ÿæä¾›æç¤º
if not os.path.exists(db_path):
    st.warning(f"æ‰¾ä¸åˆ° DB æª”ï¼š{db_path}\n\nå¦‚æœä½ æ˜¯åœ¨ dashboard é¦–é åšã€ŒåŒæ­¥è³‡æ–™åº«ã€ï¼Œè«‹å…ˆåŒæ­¥å¾Œå†ä¾† DB Checkã€‚")

# å¿«é€Ÿ stats
if os.path.exists(db_path):
    stats = get_quick_stats(db_path)
    a, b, c, d, e, f = st.columns(6)
    a.metric("Tables", int(stats.get("tables") or 0))
    b.metric("stock_prices", int(stats.get("stock_prices_rows") or 0))
    c.metric("stock_analysis", int(stats.get("stock_analysis_rows") or 0))
    d.metric("limit_up_events", int(stats.get("limit_up_events_rows") or 0))
    e.metric("daytrade_events", int(stats.get("daytrade_events_rows") or 0))
    f.metric("year_contribution", int(stats.get("year_contribution_rows") or 0))

st.divider()

if os.path.exists(db_path):
    try:
        tables = list_tables(db_path)
    except Exception as e:
        st.error(f"ç„¡æ³•è®€å– DB tablesï¼š{e}")
        st.stop()

    if not tables:
        st.info("DB å…§æ²’æœ‰ä»»ä½• tablesï¼ˆæˆ–åªæœ‰ sqlite ç³»çµ±è¡¨ï¼‰ã€‚")
        st.stop()

    dd_map = _build_default_dictionary()

    left, right = st.columns([1.0, 2.2], vertical_alignment="top")

    with left:
        table = st.selectbox("é¸æ“‡ table", tables)
        show_all_tables = st.checkbox("ä¸€æ¬¡å±•é–‹æ‰€æœ‰ tablesï¼ˆå¯èƒ½å¾ˆæ…¢ï¼‰", value=False)

        st.markdown("#### æ“ä½œå°æŠ„")
        st.markdown(
            "- **Schema**ï¼šPRAGMA table_info\n"
            "- **Sample**ï¼šSELECT * LIMIT N\n"
            "- **Dictionary**ï¼šå¯åœ¨ç¨‹å¼ç¢¼ä¸­è£œé½Šï¼Œæˆ–ä½ ä¹Ÿå¯ä»¥æŠŠ dict å€å¡Šæ¬å»ç¨ç«‹æª”\n"
            "- **AI Prompt**ï¼šåº•ä¸‹æœƒè‡ªå‹•ç”¢ç”Ÿï¼Œç›´æ¥è¤‡è£½è²¼ä¸Šå³å¯"
        )

    def render_one_table(tname: str):
        schema_df = read_table_schema(db_path, tname)
        head_df = read_table_head(db_path, tname, n=int(head_n))
        dict_df = render_data_dictionary(tname, schema_df, dd_map)

        st.subheader(f"ğŸ“‹ {tname}")

        t1, t2 = st.tabs(["Schema & Sample", "Data Dictionary & AI Prompt"])

        with t1:
            st.markdown("**Schema**")
            st.dataframe(schema_df, use_container_width=True)

            st.markdown(f"**Sample head (top {int(head_n)})**")
            st.dataframe(head_df, use_container_width=True)

        with t2:
            st.markdown("**æ¬„ä½ä¸­è‹±æ–‡è§£é‡‹ï¼ˆå¯è‡ªè¡Œè£œé½Šï¼‰**")
            st.dataframe(dict_df, use_container_width=True)

            st.markdown("**å¯ç›´æ¥è²¼ä¸Šå• AI çš„ prompt**ï¼ˆ`st.code` å³ä¸Šè§’é€šå¸¸å¯ä¸€éµè¤‡è£½ï¼‰")
            prompt = build_ai_prompt(db_path, tname, schema_df, head_df, dict_df)
            st.code(prompt, language="markdown")

    with right:
        if show_all_tables:
            for tname in tables:
                with st.expander(tname, expanded=False):
                    render_one_table(tname)
        else:
            render_one_table(table)

else:
    st.info("è«‹å…ˆæº–å‚™/åŒæ­¥ SQLite DB æª”æ¡ˆï¼Œå†ä¾†ä½¿ç”¨ DB Checkã€‚")
