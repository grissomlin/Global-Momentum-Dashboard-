# pages/dbcheck.py
# -*- coding: utf-8 -*-
import os
import sqlite3
import pandas as pd
import streamlit as st

# -----------------------------
# Helpers
# -----------------------------
def walk_find_db_files(search_roots, max_files=200):
    found = []
    seen = set()

    for root in search_roots:
        if not root or not os.path.exists(root):
            continue
        # å¦‚æœ root æœ¬èº«æ˜¯æª”æ¡ˆ
        if os.path.isfile(root) and root.lower().endswith(".db"):
            p = os.path.abspath(root)
            if p not in seen:
                found.append(p); seen.add(p)
            continue

        # éè¿´æƒè³‡æ–™å¤¾
        for dirpath, _, filenames in os.walk(root):
            for fn in filenames:
                if fn.lower().endswith(".db"):
                    p = os.path.abspath(os.path.join(dirpath, fn))
                    if p not in seen:
                        found.append(p); seen.add(p)
                        if len(found) >= max_files:
                            return sorted(found)
    return sorted(found)

def get_tables(conn):
    q = "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%' ORDER BY name"
    return [r[0] for r in conn.execute(q).fetchall()]

def get_columns(conn, table):
    rows = conn.execute(f"PRAGMA table_info({table})").fetchall()
    return [{"name": r[1], "type": r[2], "notnull": r[3], "pk": r[5]} for r in rows]

def get_count(conn, table):
    return conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0]

def read_head(conn, table, n):
    return pd.read_sql(f"SELECT * FROM {table} LIMIT {int(n)}", conn)

def make_ai_prompt(table, cols, sample_df):
    col_lines = "\n".join([f"- {c['name']} ({c['type']})" for c in cols])
    sample_csv = sample_df.to_csv(index=False)
    return f"""ä½ æ˜¯ä¸€ä½è³‡æ–™å·¥ç¨‹/é‡åŒ–ç ”ç©¶åŠ©ç†ã€‚è«‹å”åŠ©æˆ‘ç†è§£ SQLite è³‡æ–™è¡¨æ¬„ä½å®šç¾©èˆ‡ç”¨é€”ï¼Œä¸¦æª¢æŸ¥æ˜¯å¦æœ‰ç¼ºæ¬„ã€å‹åˆ¥ä¸ä¸€è‡´æˆ–å¯ç–‘è³‡æ–™å“è³ªå•é¡Œã€‚

ã€Tableã€‘{table}

ã€Schemaã€‘
{col_lines}

ã€Top rows (CSV)ã€‘
{sample_csv}

è«‹è¼¸å‡ºï¼š
1) æ¯å€‹æ¬„ä½ç”¨é€”æ¨æ¸¬ï¼ˆä¸­è‹±æ–‡ï¼‰
2) æœ€å¯èƒ½éœ€è¦è£œå……/ä¿®æ­£çš„æ¬„ä½èˆ‡ç†ç”±
3) å»ºè­°æˆ‘å¾ŒçºŒç”¨å“ªäº› SQL/æŸ¥è©¢å»é©—è­‰è³‡æ–™å“è³ª
"""

# -----------------------------
# UI
# -----------------------------
st.set_page_config(page_title="DB Check", layout="wide")
st.title("ğŸ§ª DB Checkï¼ˆè³‡æ–™åº«è®€å– + Schema + Sample + æ¬„ä½å­—å…¸ + AI Promptï¼‰")
st.caption("ç”¨é€”ï¼šç¢ºèª SQLite DB å¯æ­£å¸¸è®€å–ã€å¿«é€Ÿçœ‹åˆ°æ¯å€‹ table çš„æ¬„ä½èˆ‡å‰ N ç­†ï¼Œä¸¦ç”Ÿæˆå¯ç›´æ¥è²¼å»å• AI çš„ promptã€‚")

cwd = os.getcwd()

# ä½ åœ¨ Render å¸¸è¦‹è·¯å¾‘ï¼š/opt/render/project/srcï¼ˆç¨‹å¼ï¼‰+ /opt/render/projectï¼ˆä¸Šå±¤ï¼‰+ /tmpï¼ˆæš«å­˜ï¼‰
search_roots = [
    st.session_state.get("db_path", ""),  # è‹¥å…¶ä»–é é¢å·²è¨­ db_pathï¼Œå„ªå…ˆæ”¾é€²ä¾†
    cwd,
    "/opt/render/project/src",
    "/opt/render/project",
    "/tmp",
]

with st.expander("ğŸ” ç’°å¢ƒè³‡è¨Š / DB æƒæ", expanded=True):
    st.write("ç•¶å‰ç›®éŒ„:", cwd)
    st.write("æƒæ roots:", search_roots)

    # é¡å¤–ï¼šåˆ—å‡º cwd æª”æ¡ˆï¼Œè®“ä½ ä¸€çœ¼çœ‹åˆ° DB åˆ°åº•æœ‰æ²’æœ‰åœ¨é€™å±¤
    try:
        cwd_files = sorted(os.listdir(cwd))
        st.write(f"cwd æª”æ¡ˆæ•¸: {len(cwd_files)}")
        st.code("\n".join(cwd_files[:200]) + ("\n...(truncated)" if len(cwd_files) > 200 else ""))
    except Exception as e:
        st.warning(f"ç„¡æ³•åˆ—å‡º cwd æª”æ¡ˆï¼š{e}")

db_files = walk_find_db_files(search_roots)

# æ‰‹å‹•è¼¸å…¥è·¯å¾‘ï¼ˆè¶…é‡è¦ï¼Œæ•‘å‘½ç”¨ï¼‰
manual_path = st.text_input("ï¼ˆå¯é¸ï¼‰æ‰‹å‹•è¼¸å…¥ DB çµ•å°è·¯å¾‘", value="")

# å¦‚æœæ‰‹å‹•è¼¸å…¥æœ‰æ•ˆï¼Œç›´æ¥ç”¨
picked_db = None
if manual_path and os.path.exists(manual_path) and manual_path.lower().endswith(".db"):
    picked_db = os.path.abspath(manual_path)

# å¦å‰‡ç”¨æƒæçµæœ
if picked_db is None:
    if not db_files:
        st.error("æ‰¾ä¸åˆ°ä»»ä½• .db æª”æ¡ˆã€‚ğŸ‘‰ é€™ä»£è¡¨ DB ä¸åœ¨æƒæè·¯å¾‘å…§æˆ–åŒæ­¥å…¶å¯¦æ²’è½åœ°åˆ°é€™å€‹ containerã€‚")
        st.info("å»ºè­°ï¼šæŠŠä½ ã€åŒæ­¥ DBã€é‚£æ®µç¨‹å¼ä¸‹è¼‰çš„å¯¦éš›è·¯å¾‘ print å‡ºä¾†ï¼ˆæˆ–åœ¨åŒæ­¥å¾ŒæŠŠ db_path å¯«å…¥ st.session_state['db_path']ï¼‰ã€‚")
        st.stop()

    # é è¨­æŒ‘ tw_stock_warehouse.db
    default_idx = 0
    for i, p in enumerate(db_files):
        if os.path.basename(p) == "tw_stock_warehouse.db":
            default_idx = i
            break

    picked_db = st.selectbox("SQLite DBï¼ˆéè¿´æƒæçµæœï¼‰", db_files, index=default_idx)

# å…¨ç«™å…±ç”¨
st.session_state["db_path"] = picked_db
st.session_state["db_name"] = os.path.basename(picked_db)

st.success(f"âœ… ä½¿ç”¨ DBï¼š{picked_db}")

n_head = st.number_input("æ¯è¡¨é¡¯ç¤ºå‰ N ç­†", min_value=1, max_value=200, value=10, step=1)

# æ¬„ä½å­—å…¸ï¼ˆå¯æ…¢æ…¢æ“´å……ï¼‰
COLUMN_DICT = {
    ("stock_prices", "symbol"): {"zh": "è‚¡ç¥¨ä»£è™Ÿ", "en": "Symbol/Ticker"},
    ("stock_prices", "date"): {"zh": "äº¤æ˜“æ—¥æœŸ", "en": "Trading date"},
    ("stock_prices", "open"): {"zh": "é–‹ç›¤åƒ¹", "en": "Open price"},
    ("stock_prices", "high"): {"zh": "æœ€é«˜åƒ¹", "en": "High price"},
    ("stock_prices", "low"): {"zh": "æœ€ä½åƒ¹", "en": "Low price"},
    ("stock_prices", "close"): {"zh": "æ”¶ç›¤åƒ¹", "en": "Close price"},
    ("stock_prices", "volume"): {"zh": "æˆäº¤é‡", "en": "Volume"},
    ("stock_info", "market"): {"zh": "å¸‚å ´ä»£ç¢¼", "en": "Market code"},
    ("stock_info", "sector"): {"zh": "ç”¢æ¥­åˆ¥", "en": "Sector"},
    ("stock_analysis", "is_limit_up"): {"zh": "æ˜¯å¦æ¼²åœ(æ”¶ç›¤)", "en": "Is limit-up at close"},
    ("stock_analysis", "lu_type"): {"zh": "æ¼²åœå‹æ…‹", "en": "Limit-up pattern type"},
    ("stock_analysis", "consecutive_limits"): {"zh": "é€£æ¿å¤©æ•¸", "en": "Consecutive limit-up days"},
    ("stock_analysis", "strength_rank"): {"zh": "å¼·åº¦åˆ†ç®±æ¨™ç±¤", "en": "Strength rank bin label"},
    ("stock_analysis", "strength_value"): {"zh": "å¼·åº¦æ•¸å€¼åŒ–", "en": "Strength numeric value"},
}

conn = sqlite3.connect(picked_db, timeout=60)
try:
    tables = get_tables(conn)
    if not tables:
        st.warning("DB å…§æ²’æœ‰ä»»ä½• tableã€‚")
        st.stop()

    st.subheader("ğŸ“š Tables")
    table = st.selectbox("é¸æ“‡ Table", tables, index=0)

    total = get_count(conn, table)
    st.write(f"**{table}** | rows: **{total:,}**")

    cols_meta = get_columns(conn, table)

    st.markdown("### ğŸ§± Schema")
    st.dataframe(pd.DataFrame(cols_meta), use_container_width=True)

    st.markdown("### ğŸ”Ÿ Sample (Top N rows)")
    sample_df = read_head(conn, table, n_head)
    st.dataframe(sample_df, use_container_width=True)

    st.markdown("### ğŸ“– æ¬„ä½å­—å…¸ï¼ˆä¸­è‹±æ–‡ï¼‰")
    dict_rows = []
    for c in cols_meta:
        key = (table, c["name"])
        d = COLUMN_DICT.get(key, {})
        dict_rows.append({
            "column": c["name"],
            "type": c["type"],
            "zh": d.get("zh", ""),
            "en": d.get("en", ""),
            "note": d.get("note", ""),
        })
    st.dataframe(pd.DataFrame(dict_rows), use_container_width=True)

    st.markdown("### ğŸ¤– ä¸€éµç”Ÿæˆã€Œå¯è²¼å»å• AIã€çš„ Prompt")
    st.text_area("Promptï¼ˆå¯ç›´æ¥è¤‡è£½ï¼‰", make_ai_prompt(table, cols_meta, sample_df), height=260)

finally:
    conn.close()
