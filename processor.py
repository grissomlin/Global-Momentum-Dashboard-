# processor.py
# -*- coding: utf-8 -*-
"""
processor.py
------------
Feature Layerï¼ˆå¯«å› stock_analysisï¼‰

âœ… ç›®æ¨™ï¼š
- åªè² è²¬æŠŠ is_limit_up / strength_rank(10%èµ·æ¯10%åˆ†ç®±åˆ°100%+) / lu_type / consecutive_limits
  + æŠ€è¡“æŒ‡æ¨™ + å¹´åº¦å·”å³°è²¢ç»åº¦ ç­‰ features å¯«å› stock_analysis

âœ… è¨­è¨ˆåŸå‰‡ï¼š
- å¸‚å ´è¦å‰‡é›†ä¸­åœ¨ market_rules.pyï¼ˆTW/CN/JP çš„ limit åˆ¤å®š + tick + åˆ†ç®± intervalsï¼‰
- processor.py ä¸ç¡¬å¯«å„å¸‚å ´è¦å‰‡ï¼šç›¡é‡é€é market_rules.get_rule(...) / market_rules.calc_limit_up_price(...)
- è‹¥ market_rules.py å°šæœªå®Œæˆï¼Œprocessor ä»æœ‰ fallbackï¼ˆå¯è·‘ï¼Œä½† TW/JP æ¼²åœç²¾æº–åº¦æœƒè¼ƒå·®ï¼‰

âš ï¸ æ³¨æ„ï¼š
- äº‹ä»¶è¡¨ï¼ˆæ¼²åœå‹æ…‹ / éš”æ—¥æ²– / æœªä¾†å ±é…¬ï¼‰è«‹æ”¾ event_engine.py
  processor.py ä¸åšäº‹ä»¶è¡¨ã€ä¸åš future returnsã€‚
"""

import sqlite3
import pandas as pd
import numpy as np
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)

SQLITE_TIMEOUT = 120


# =============================================================================
# 0) åŒ¯å…¥å¸‚å ´è¦å‰‡ï¼ˆä¸»è·¯å¾‘ï¼‰ï¼Œè‹¥æ²’æœ‰å‰‡ fallback
# =============================================================================
try:
    import market_rules  # ä½ æœƒå¦å¤–æä¾›ï¼šTW/CN/JP limit åˆ¤å®š + tick + bins
    HAS_MARKET_RULES = True
except Exception:
    market_rules = None
    HAS_MARKET_RULES = False


# =============================================================================
# 1) Fallback è¦å‰‡ï¼ˆåªæœ‰ç•¶ market_rules.py ä¸å­˜åœ¨æ™‚æ‰ç”¨ï¼‰
# =============================================================================
def _fallback_get_rule(market: str, market_detail: str, symbol: str) -> dict:
    """
    å›å‚³ dictï¼š
    - limit_kind: 'pct' / 'none'
    - limit_up_pct: float or None
    - threshold: floatï¼ˆå¼·å‹¢æ—¥é–€æª»ï¼Œçµ¦ peak_contribution ç”¨ï¼‰
    - strength_edges: list of edges in % for pd.cut
    - strength_labels: labels for bins
    - max_strength: int
    """
    m = (market or "").upper().strip()
    md = (market_detail or "").lower().strip()
    sym = (symbol or "").upper().strip()

    edges = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, np.inf]
    labels = [
        "RANK_0_10", "RANK_10_20", "RANK_20_30", "RANK_30_40", "RANK_40_50",
        "RANK_50_60", "RANK_60_70", "RANK_70_80", "RANK_80_90", "RANK_90_100", "RANK_100UP",
    ]

    # --- TW ---
    if m in ["TW", "TSE", "GTSM"] or sym.endswith(".TW") or sym.endswith(".TWO"):
        if md == "emerging":
            # èˆˆæ«ƒï¼šç„¡æ¼²è·Œå¹…é™åˆ¶ï¼ˆfallbackï¼šä¸åšæ¼²åœåˆ¤å®šï¼‰
            return dict(limit_kind="none", limit_up_pct=None, threshold=0.20,
                        strength_edges=edges, strength_labels=labels, max_strength=100)
        else:
            # ä¸Šå¸‚/ä¸Šæ«ƒï¼š10% æ¼²åœï¼ˆfallbackï¼šä¸åš tick å°é½Šï¼‰
            return dict(limit_kind="pct", limit_up_pct=0.10, threshold=0.10,
                        strength_edges=edges, strength_labels=labels, max_strength=100)

    # --- CN ---
    if m in ["SSE", "SZSE", "CN", "CHINA"] or sym.endswith(".SS") or sym.endswith(".SZ"):
        # 300/301/688 => 20%ï¼ˆå‰µæ¥­æ¿/ç§‘å‰µæ¿ï¼‰ï¼Œå…¶ä»– 10%
        code = "".join([c for c in sym if c.isdigit()])
        up = 0.20 if code.startswith(("300", "301", "688")) else 0.10
        return dict(limit_kind="pct", limit_up_pct=up, threshold=up,
                    strength_edges=edges, strength_labels=labels, max_strength=100)

    # --- JP (fallbackï¼šç•¶ä½œç„¡æ¼²è·Œå¹…é™åˆ¶ï¼Œä¸ç®—æ¼²åœ) ---
    if m in ["JP", "TSE", "JPX"] or sym.endswith(".T"):
        return dict(limit_kind="none", limit_up_pct=None, threshold=0.10,
                    strength_edges=edges, strength_labels=labels, max_strength=100)

    # --- Default ---
    return dict(limit_kind="none", limit_up_pct=None, threshold=0.10,
                strength_edges=edges, strength_labels=labels, max_strength=100)


def _fallback_calc_limit_up_price(prev_close: pd.Series, limit_up_pct: float) -> pd.Series:
    # fallbackï¼šä¸å°é½Š tick
    return (prev_close * (1 + limit_up_pct)).round(2)


# =============================================================================
# 2) å…±ç”¨å·¥å…·ï¼šåˆ†ç®±ã€é€£æ¿ã€LU å‹æ…‹ï¼ˆä½ æ–‡ç« è¦å‰‡ï¼‰
# =============================================================================
def _make_strength_bins(change_pct: pd.Series, edges, labels) -> pd.Series:
    """
    change_pctï¼šç™¾åˆ†æ¯”ï¼ˆä¾‹å¦‚ 12.3 è¡¨ç¤º +12.3%ï¼‰
    edgesï¼šä¾‹å¦‚ [0,10,20,...,100,inf]
    labelsï¼šå°æ‡‰ bins
    """
    out = pd.cut(change_pct, bins=edges, labels=labels, right=False, include_lowest=True)
    out = out.astype("object")
    out = np.where(change_pct <= 0, "NEGATIVE", out)
    out = np.where((change_pct > 0) & (change_pct < edges[1]), "POSITIVE", out)  # 0~10% æ­£å€¼
    return pd.Series(out, index=change_pct.index)


def _strength_value_from_rank(rank: pd.Series) -> pd.Series:
    """
    å–åˆ†ç®±çš„æ•¸å€¼è¡¨ç¤ºï¼ˆä¾‹å¦‚ RANK_30_40 => 30, RANK_100UP => 100, POSITIVE=>1, NEGATIVE=>0ï¼‰
    """
    def _v(x):
        if x in ("NEGATIVE", None) or (isinstance(x, float) and np.isnan(x)):
            return 0
        if x == "POSITIVE":
            return 1
        if isinstance(x, str) and x.startswith("RANK_"):
            if x.endswith("UP"):
                digits = "".join([c for c in x if c.isdigit()])
                return int(digits) if digits else 0
            parts = x.replace("RANK_", "").split("_")
            try:
                return int(parts[0])
            except Exception:
                return 0
        return 0

    return rank.apply(_v)


def _compute_consecutive_limits(is_limit_up: pd.Series) -> pd.Series:
    """
    é€£æ¿ï¼šåªåœ¨ is_limit_up==1 æ™‚è¨ˆç®—é€£çºŒå¤©æ•¸ï¼Œå…¶ä»–ç‚º 0
    """
    grp = (is_limit_up != is_limit_up.shift(1)).cumsum()
    streak = is_limit_up.groupby(grp).cumsum()
    return np.where(is_limit_up == 1, streak, 0)


def _compute_lu_type_article_style(
    is_limit_up: pd.Series,
    open_: pd.Series,
    high: pd.Series,
    low: pd.Series,
    close: pd.Series,
    prev_close: pd.Series,
    volume: pd.Series,
    vol_ma5: pd.Series,
) -> pd.Series:
    """
    ä¾ä½ æ–‡ç« çš„ã€Œè‡ªå‹•åŒ–åˆ†é¡ã€ï¼š
    1) is_gap  : (Open/Prev_Close - 1) >= 0.07
    2) is_high : Volume/Vol_MA5 >= 3
    3) is_low  : Volume/Vol_MA5 <= 0.4
    4) is_float: é gap ä¸” (Close/Open - 1) >= 0.05
    å„ªå…ˆåºï¼š
    1. GAP_UP_LOCK   ï¼šis_gap ä¸” is_low_vol
    2. GAP_UP        ï¼šis_gap ä¸” é is_low_vol
    3. FLOAT_HV      ï¼šis_float ä¸” is_high_vol
    4. FLOAT         ï¼šis_float ä¸” é is_high_vol
    5. LOW_VOL_LOCK  ï¼šis_low_vol
    6. HIGH_VOL_LOCK ï¼šis_high_vol
    7. OTHER
    åˆä½µæˆäº”é¡ï¼š
      FLOATING / GAP_UP / OTHER / HIGH_VOLUME_LOCK / NO_VOLUME_LOCK
    """
    safe_prev = prev_close.replace(0, np.nan)
    safe_vol_ma5 = vol_ma5.replace(0, np.nan)
    safe_open = open_.replace(0, np.nan)

    gap = (open_ / safe_prev - 1) >= 0.07
    vol_ratio = volume / safe_vol_ma5
    high_vol = vol_ratio >= 3
    low_vol = vol_ratio <= 0.4
    floating = (~gap) & ((close / safe_open - 1) >= 0.05)

    cat = pd.Series("OTHER", index=is_limit_up.index, dtype="object")
    cat = np.where(gap & low_vol, "GAP_UP_LOCK", cat)
    cat = np.where(gap & (~low_vol), "GAP_UP", cat)
    cat = np.where(floating & high_vol, "FLOAT_HV", cat)
    cat = np.where(floating & (~high_vol), "FLOAT", cat)
    cat = np.where((~gap) & (~floating) & low_vol, "LOW_VOL_LOCK", cat)
    cat = np.where((~gap) & (~floating) & high_vol, "HIGH_VOL_LOCK", cat)

    merged = pd.Series("OTHER", index=is_limit_up.index, dtype="object")
    merged = np.where(np.isin(cat, ["FLOAT", "FLOAT_HV"]), "FLOATING", merged)
    merged = np.where(np.isin(cat, ["GAP_UP", "GAP_UP_LOCK"]), "GAP_UP", merged)
    merged = np.where(cat == "HIGH_VOL_LOCK", "HIGH_VOLUME_LOCK", merged)
    merged = np.where(cat == "LOW_VOL_LOCK", "NO_VOLUME_LOCK", merged)

    merged = np.where(is_limit_up == 1, merged, None)
    return pd.Series(merged, index=is_limit_up.index, dtype="object")


# =============================================================================
# 3) ä¸»æµç¨‹
# =============================================================================
def process_market_data(db_path: str):
    conn = sqlite3.connect(db_path, timeout=SQLITE_TIMEOUT)

    # 1) è®€å–æ•¸æ“šä¸¦é—œè¯ stock_info å–å¾—å¸‚å ´èˆ‡ç”¢æ¥­åˆ¥
    query = """
    SELECT p.*, i.market, i.sector, i.market_detail
    FROM stock_prices p
    LEFT JOIN stock_info i ON p.symbol = i.symbol
    """
    df = pd.read_sql(query, conn)

    if df.empty:
        print("âŒ æ²’æœ‰æ‰¾åˆ°è‚¡ç¥¨æ•¸æ“š")
        conn.close()
        return

    # å¿…è¦æ¬„ä½ä¿åº•ï¼ˆé¿å…æŸäº›å¸‚å ´ç¼º low/high/volumeï¼‰
    for c in ["open", "high", "low", "close", "volume"]:
        if c not in df.columns:
            df[c] = np.nan

    if "market" not in df.columns:
        df["market"] = ""
    if "market_detail" not in df.columns:
        df["market_detail"] = ""
    if "sector" not in df.columns:
        df["sector"] = None

    # å‹åˆ¥æ¸…ç†
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df = df.dropna(subset=["date"])
    df = df.sort_values(["symbol", "date"]).reset_index(drop=True)

    for c in ["open", "high", "low", "close", "volume"]:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    processed_list = []

    for symbol, group in df.groupby("symbol", sort=False):
        group = group.copy().sort_values("date").reset_index(drop=True)

        # å¤ªçŸ­ä¸åšï¼ˆé¿å…æŒ‡æ¨™å™ªè²å¾ˆå¤§ï¼‰
        if len(group) < 40:
            continue

        market = str(group["market"].iloc[0]) if "market" in group.columns else ""
        market_detail = str(group["market_detail"].iloc[0]) if "market_detail" in group.columns else ""

        # å–å¾—å¸‚å ´è¦å‰‡ï¼ˆä»¥ market_rules.py ç‚ºä¸»ï¼‰
        if HAS_MARKET_RULES and hasattr(market_rules, "get_rule"):
            rule = market_rules.get_rule(market=market, market_detail=market_detail, symbol=symbol)
        else:
            rule = _fallback_get_rule(market, market_detail, symbol)

        # --- åŸºç¤æ¬„ä½ ---
        group["prev_close"] = group["close"].shift(1)
        group["daily_change"] = group["close"].pct_change()
        group["avg_vol_20"] = group["volume"].rolling(window=20, min_periods=1).mean()
        group["vol_ma5"] = group["volume"].rolling(window=5, min_periods=1).mean()
        group["year"] = group["date"].dt.year

        # --- æ¼²å¹…ç™¾åˆ†æ¯” ---
        change_pct = (group["daily_change"] * 100).astype(float)

        # --- strength_rank / strength_valueï¼ˆ10%èµ·æ¯10%åˆ°100%+ï¼‰ ---
        edges = rule.get("strength_edges")
        labels = rule.get("strength_labels")
        if edges is None or labels is None:
            edges = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, np.inf]
            labels = [
                "RANK_0_10", "RANK_10_20", "RANK_20_30", "RANK_30_40", "RANK_40_50",
                "RANK_50_60", "RANK_60_70", "RANK_70_80", "RANK_80_90", "RANK_90_100", "RANK_100UP",
            ]

        group["strength_rank"] = _make_strength_bins(change_pct, edges, labels)
        group["strength_value"] = _strength_value_from_rank(group["strength_rank"]).astype(int)

        # --- æ¼²åœåˆ¤å®š is_limit_up ---
        group["is_limit_up"] = 0
        limit_kind = rule.get("limit_kind", "none")
        limit_up_pct = rule.get("limit_up_pct", None)

        # market_rules ç²¾æº–ç‰ˆï¼ˆTW tick / JP å€¤å¹…åˆ¶é™ï¼‰
        used_precise = False
        if HAS_MARKET_RULES and hasattr(market_rules, "calc_limit_up_price"):
            try:
                limit_price = market_rules.calc_limit_up_price(
                    prev_close=group["prev_close"].astype(float),
                    market=market,
                    market_detail=market_detail,
                    symbol=symbol,
                )
                if limit_price is not None:
                    if hasattr(market_rules, "tick_size"):
                        tick = group["prev_close"].astype(float).apply(
                            lambda x: market_rules.tick_size(float(x), market=market, symbol=symbol)
                        )
                        buffer = tick.fillna(0) * 0.5
                    else:
                        buffer = 0.0
                    group["is_limit_up"] = (group["close"].astype(float) >= (limit_price.astype(float) - buffer)).astype(int)
                    used_precise = True
            except Exception:
                used_precise = False

        # fallbackï¼šå›ºå®šç™¾åˆ†æ¯”ï¼ˆåƒ…åœ¨ç²¾æº–åˆ¤å®šæ²’ç”Ÿæ•ˆæ™‚ï¼‰
        if (not used_precise) and limit_kind == "pct" and isinstance(limit_up_pct, (int, float)):
            limit_price = _fallback_calc_limit_up_price(group["prev_close"].astype(float), float(limit_up_pct))
            group["is_limit_up"] = (group["close"].astype(float) >= limit_price * 0.999).astype(int)

        # å…¶é¤˜ï¼šç„¡æ¼²è·Œå¹…é™åˆ¶ï¼ˆprocessor ä¸æŠŠ 10% ç•¶äº‹ä»¶æ¼²åœï¼›äº‹ä»¶åœ¨ event_engineï¼‰
        synth = rule.get("synthetic_limit_up_pct", None)
        if synth is not None and group["is_limit_up"].sum() == 0:
            group["is_limit_up"] = (group["daily_change"].astype(float) >= float(synth)).astype(int)

        # --- ä¸€å­—é– ---
        group["is_one_tick_lock"] = (
            (group["open"] == group["close"]) &
            (group["high"] == group["low"]) &
            (group["high"] == group["close"])
        ).astype(int)

        # --- LU å‹æ…‹ï¼ˆåªåœ¨æ¼²åœæ—¥çµ¦é¡å‹ï¼‰ ---
        group["lu_type"] = _compute_lu_type_article_style(
            is_limit_up=group["is_limit_up"],
            open_=group["open"].astype(float),
            high=group["high"].astype(float),
            low=group["low"].astype(float),
            close=group["close"].astype(float),
            prev_close=group["prev_close"].astype(float),
            volume=group["volume"].astype(float),
            vol_ma5=group["vol_ma5"].astype(float),
        )

        # --- é€£æ¿æ¬¡æ•¸ ---
        group["consecutive_limits"] = _compute_consecutive_limits(group["is_limit_up"]).astype(int)

        # --- å¹´åº¦å·”å³°è²¢ç»åº¦ï¼ˆç”¨ rule['threshold']ï¼‰ ---
        threshold = float(rule.get("threshold", 0.10))

        def calc_peak_contribution(df_year: pd.DataFrame) -> pd.DataFrame:
            if df_year.empty:
                df_year["peak_date"] = None
                df_year["peak_high_ret"] = np.nan
                df_year["strong_day_contribution"] = 0.0
                return df_year

            valid_high = pd.to_numeric(df_year["high"], errors="coerce").dropna()
            if valid_high.empty:
                df_year["peak_date"] = None
                df_year["peak_high_ret"] = np.nan
                df_year["strong_day_contribution"] = 0.0
                return df_year

            peak_idx = valid_high.idxmax()
            peak_date = df_year.loc[peak_idx, "date"] if peak_idx in df_year.index else None
            peak_price = df_year.loc[peak_idx, "high"] if peak_idx in df_year.index else np.nan

            year_open = df_year.iloc[0]["open"] if len(df_year) > 0 else np.nan

            if pd.notna(peak_price) and pd.notna(year_open) and float(year_open) > 0:
                total_peak_log = float(np.log(float(peak_price) / float(year_open)))
            else:
                total_peak_log = 0.0

            if peak_date is not None:
                mask_before = (df_year["date"] <= peak_date)
            else:
                mask_before = pd.Series(False, index=df_year.index)

            # logretï¼šclose/prev_closeï¼ˆé¿å… prev_close=0ï¼‰
            safe_prev = pd.to_numeric(df_year["prev_close"], errors="coerce").replace(0, np.nan)
            safe_close = pd.to_numeric(df_year["close"], errors="coerce").replace(0, np.nan)
            daily_logs = np.log(safe_close / safe_prev).replace([np.inf, -np.inf], np.nan).fillna(0.0)

            strong_day_mask = (pd.to_numeric(df_year["daily_change"], errors="coerce").fillna(0.0) >= threshold) & mask_before

            if strong_day_mask.any() and total_peak_log > 0:
                strong_contribution = float(daily_logs[strong_day_mask].sum())
                strong_day_contribution = float(strong_contribution / total_peak_log * 100)
            else:
                strong_day_contribution = 0.0

            df_year["peak_date"] = peak_date
            df_year["peak_high_ret"] = (
                (float(peak_price) - float(year_open)) / float(year_open) * 100
                if pd.notna(peak_price) and pd.notna(year_open) and float(year_open) > 0
                else np.nan
            )
            df_year["strong_day_contribution"] = strong_day_contribution
            return df_year

        year_values = group["year"].copy()
        try:
            group = group.groupby("year", group_keys=False).apply(calc_peak_contribution, include_groups=False)
        except TypeError:
            group = group.groupby("year", group_keys=False).apply(calc_peak_contribution)

        if "year" not in group.columns:
            group["year"] = year_values

        # --- æŠ€è¡“æŒ‡æ¨™ ---
        group["ma20"] = group["close"].rolling(window=20, min_periods=1).mean()
        group["ma60"] = group["close"].rolling(window=60, min_periods=1).mean()

        ema12 = group["close"].ewm(span=12, adjust=False).mean()
        ema26 = group["close"].ewm(span=26, adjust=False).mean()
        group["macd"] = ema12 - ema26
        group["macds"] = group["macd"].ewm(span=9, adjust=False).mean()
        group["macdh"] = group["macd"] - group["macds"]

        # å¹´åŒ–æ³¢å‹•ç‡ï¼ˆ20Dï¼‰
        group["volatility_20"] = group["daily_change"].rolling(window=20, min_periods=1).std() * np.sqrt(252)

        # RSI 14
        delta = group["close"].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=1).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=1).mean()
        rs = gain / loss.replace(0, np.nan)
        group["rsi"] = 100 - (100 / (1 + rs))

        group["volume_ratio"] = group["volume"] / group["avg_vol_20"].replace(0, np.nan)

        rolling_20_high = group["high"].rolling(window=20, min_periods=1).max()
        rolling_20_low = group["low"].rolling(window=20, min_periods=1).min()
        denom = (rolling_20_high - rolling_20_low).replace(0, np.nan)
        group["price_position_20"] = (group["close"] - rolling_20_low) / denom

        # YTD Retï¼ˆç”¨æ”¶ç›¤ï¼‰
        year_start_prices = group.groupby("year")["close"].first()
        year_to_start = year_start_prices.to_dict()
        group["year_start_price"] = group["year"].map(year_to_start)
        group["ytd_ret"] = ((group["close"] - group["year_start_price"]) / group["year_start_price"] * 100).round(2)

        processed_list.append(group)

    if not processed_list:
        print("âŒ æ²’æœ‰è™•ç†å¾Œçš„æ•¸æ“šï¼ˆå¯èƒ½æ˜¯è³‡æ–™å¤ªå°‘æˆ–æ¬„ä½ç¼ºå¤±ï¼‰")
        conn.close()
        return

    df_final = pd.concat(processed_list, ignore_index=True)

    # æ—¥æœŸè½‰æ–‡å­—ï¼ˆSQLite å¯«å…¥ç©©å®šï¼‰
    df_final["date"] = pd.to_datetime(df_final["date"]).dt.strftime("%Y-%m-%d")
    if "peak_date" in df_final.columns:
        df_final["peak_date"] = pd.to_datetime(df_final["peak_date"], errors="coerce").dt.strftime("%Y-%m-%d")

    # é‡å»º stock_analysisï¼ˆâœ… é€™ä¸€æ­¥æœƒè®“ DB çœŸçš„ã€Œå¢åŠ æ¬„ä½ã€ï¼šå› ç‚º schema æœƒè·Ÿè‘— df_final è®Šï¼‰
    conn.execute("DROP TABLE IF EXISTS stock_analysis")
    df_final.to_sql("stock_analysis", conn, if_exists="replace", index=False)

    # ç´¢å¼•
    try:
        conn.execute("CREATE INDEX IF NOT EXISTS idx_symbol_date ON stock_analysis (symbol, date)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_strength_rank ON stock_analysis (strength_rank)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_market ON stock_analysis (market)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_market_detail ON stock_analysis (market_detail)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_is_limit_up ON stock_analysis (is_limit_up)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_lu_type ON stock_analysis (lu_type)")
    except Exception:
        pass

    conn.commit()

    # çµ±è¨ˆè¼¸å‡º
    total_symbols = df_final["symbol"].nunique()
    date_range = f"{df_final['date'].min()} ~ {df_final['date'].max()}"

    print("\nâœ… Feature Layer å®Œæˆï¼ˆstock_analysis å·²é‡å»ºï¼‰")
    print(f"ğŸ“Œ è‚¡ç¥¨æ•¸é‡: {total_symbols}")
    print(f"ğŸ“Œ æœŸé–“: {date_range}")
    print(f"ğŸ“Œ ç¸½è¡Œæ•¸: {len(df_final):,}")
    print("ğŸ“Œ æ–°å¢/ç¢ºèªæ¬„ä½åŒ…å«ï¼šis_limit_up, is_one_tick_lock, lu_type(æ–‡ç« è¦å‰‡), consecutive_limits, strength_rank, volatility_20")

    conn.close()


if __name__ == "__main__":
    # æ¸¬è©¦ï¼šæ”¹æˆä½ çš„ DB æª”å
    process_market_data("tw_stock_warehouse.db")
