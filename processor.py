# -*- coding: utf-8 -*-
import sqlite3
import pandas as pd
import numpy as np

def process_market_data(db_path):
    conn = sqlite3.connect(db_path)
    
    # 1. è®€å–æ•¸æ“šä¸¦é—œè¯ stock_info å–å¾—å¸‚å ´èˆ‡ç”¢æ¥­åˆ¥
    # ç¢ºä¿ä½ çš„ downloader å·²ç¶“æŠŠ 'èˆˆæ«ƒ', 'ä¸Šå¸‚', 'ä¸Šæ«ƒ' å­˜å…¥ stock_info
    query = """
    SELECT p.*, i.market, i.sector
    FROM stock_prices p
    LEFT JOIN stock_info i ON p.symbol = i.symbol
    """
    df = pd.read_sql(query, conn)
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values(['symbol', 'date'])

    processed_list = []
    
    # 2. åˆ†çµ„è¨ˆç®—
    for symbol, group in df.groupby('symbol'):
        group = group.copy().sort_values('date')
        
        # --- ğŸŸ¢ ç¬¬ä¸€æ­¥ï¼šè³‡æ–™æ¸…æ´— ---
        group['daily_change'] = group['close'].pct_change()
        # å¹³æ»‘ç•°å¸¸å€¼ (>60% ä¸”éèˆˆæ«ƒå‰‡è¦–ç‚ºç•°å¸¸)
        is_emerging = group['market'].iloc[0] == 'èˆˆæ«ƒ'
        if not is_emerging:
            group.loc[abs(group['daily_change']) > 0.6, 'close'] = np.nan
            group['close'] = group['close'].ffill()
        
        if len(group) < 40: continue 

        # åŸºç¤æ¬„ä½
        group['prev_close'] = group['close'].shift(1)
        group['avg_vol_20'] = group['volume'].rolling(window=20).mean()
        group['year'] = group['date'].dt.year
        
        # --- ğŸ”´ ç¬¬äºŒæ­¥ï¼šæ¼²åœèˆ‡é•·ç´…å€é–“æ¨™è¨˜ (LU_Type & Brackets) ---
        # æ¼²å¹…ç™¾åˆ†æ¯”
        change_pct = group['daily_change'] * 100
        
        # åˆ¤å®šæ˜¯å¦ç‚ºã€Œå—é™å¸‚å ´æ¼²åœã€
        group['is_limit_up'] = 0
        if not is_emerging:
            # ä¸Šå¸‚æ«ƒ 10% åˆ¤å®š
            group['is_limit_up'] = (group['close'] >= (group['prev_close'] * 1.0945)).astype(int)
        
        # é‡å°ã€Œç„¡é™åˆ¶ã€æˆ–ã€Œé•·ç´…æ£’ã€å®šç¾©å€é–“ (10% - 100%+)
        def label_strength(row):
            chg = row['daily_change'] * 100
            if chg >= 100: return "RANK_100UP"
            elif chg >= 50: return "RANK_50_100"
            elif chg >= 30: return "RANK_30_50"
            elif chg >= 20: return "RANK_20_30"
            elif chg >= 10: return "RANK_10_20"
            elif chg > 0:   return "POSITIVE"
            return "NEGATIVE"
        
        group['strength_rank'] = group.apply(label_strength, axis=1)

        # æ¼²åœé¡å‹ (LU_Type4)
        conditions = [
            (group['is_limit_up'] == 1) & (group['open'] == group['close']) & (group['high'] == group['low']),
            (group['is_limit_up'] == 1) & (group['open'] > group['prev_close'] * 1.05),
            (group['is_limit_up'] == 1) & (group['volume'] > group['avg_vol_20'] * 2),
            (group['is_limit_up'] == 1)
        ]
        choices = ['NO_VOLUME_LOCK', 'GAP_UP', 'HIGH_VOLUME_LOCK', 'FLOATING']
        group['lu_type'] = np.select(conditions, choices, default=None)

        # é€£æ¿æ¬¡æ•¸
        streak = group['is_limit_up'].groupby((group['is_limit_up'] != group['is_limit_up'].shift()).cumsum()).cumsum()
        group['consecutive_limits'] = np.where(group['is_limit_up'] == 1, streak, 0)

        # --- ğŸŸ£ ç¬¬ä¸‰æ­¥ï¼šå¹´åº¦å·”å³°è²¢ç»åº¦ (ä»¥æœ€é«˜åƒ¹ Peak High è¨ˆç®—) ---
        def calc_peak_contribution(df_year):
            if df_year.empty: return df_year
            peak_idx = df_year['high'].idxmax()
            peak_date = df_year.loc[peak_idx, 'date']
            peak_price = df_year.loc[peak_idx, 'high']
            year_open = df_year.iloc[0]['open']
            
            # å¹´åº¦ç¸½å·”å³°å ±é…¬ (å°æ•¸)
            total_peak_log = np.log(peak_price / year_open)
            mask_before = df_year['date'] <= peak_date
            
            # è¨ˆç®—æ‰€æœ‰ã€Œæ¼²å¹… > 10%ã€æ—¥å­çš„ç¸½è²¢ç»
            daily_logs = np.log(df_year['close'] / df_year['prev_close'])
            strong_day_mask = (df_year['daily_change'] >= 0.095) & mask_before
            strong_contribution = daily_logs[strong_day_mask].sum()
            
            df_year['peak_date'] = peak_date
            df_year['peak_high_ret'] = ((peak_price - year_open) / year_open * 100)
            df_year['strong_day_contribution'] = (strong_contribution / total_peak_log * 100) if total_peak_log > 0 else 0
            return df_year

        group = group.groupby('year', group_keys=False).apply(calc_peak_contribution)

        # --- ğŸ”µ ç¬¬å››æ­¥ï¼šåŸæœ‰æŠ€è¡“æŒ‡æ¨™ ---
        # MA
        group['ma20'] = group['close'].rolling(window=20).mean()
        group['ma60'] = group['close'].rolling(window=60).mean()
        
        # MACD
        ema12 = group['close'].ewm(span=12, adjust=False).mean()
        ema26 = group['close'].ewm(span=26, adjust=False).mean()
        group['macd'] = ema12 - ema26
        group['macds'] = group['macd'].ewm(span=9, adjust=False).mean()
        group['macdh'] = group['macd'] - group['macds']
        
        # YTD Ret (å¯¦æ¸¬æ”¶ç›¤)
        group['year_start_price'] = group.groupby('year')['close'].transform('first')
        group['ytd_ret'] = ((group['close'] - group['year_start_price']) / group['year_start_price'] * 100).round(2)

        processed_list.append(group)

    # 3. å¯«å›ä¸¦å„ªåŒ–
    df_final = pd.concat(processed_list)
    df_final.to_sql('stock_analysis', conn, if_exists='replace', index=False)
    conn.execute("CREATE INDEX IF NOT EXISTS idx_symbol_date ON stock_analysis (symbol, date)")
    conn.close()
    print("âœ… ç‰¹å¾µå·¥ç¨‹å®Œæ•´ç‰ˆå®Œæˆï¼šå«å¸‚å ´åˆ†é¡ã€é•·ç´…å¼·åº¦å€é–“ã€å·”å³°è²¢ç»åº¦ã€‚")

if __name__ == "__main__":
    process_market_data("tw_stock_warehouse.db")
