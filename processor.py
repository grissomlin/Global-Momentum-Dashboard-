# -*- coding: utf-8 -*-
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime

def process_market_data(db_path):
    conn = sqlite3.connect(db_path)
    
    # 1. è®€å–æ•¸æ“šä¸¦é—œè¯ stock_info å–å¾—å¸‚å ´èˆ‡ç”¢æ¥­åˆ¥
    # ç¢ºä¿ä½ çš„ downloader å·²ç¶“æŠŠ 'èˆˆæ«ƒ', 'ä¸Šå¸‚', 'ä¸Šæ«ƒ' å­˜å…¥ stock_info
    query = """
    SELECT p.*, i.market, i.sector
    FROM stock_prices p
    LEFT JOIN stock_info i ON p.symbol = i.symbol
    """
    df = pd.read_sql(query, conn)
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values(['symbol', 'date'])

    # æª¢æŸ¥æ˜¯å¦æœ‰æ•¸æ“š
    if df.empty:
        print("âŒ æ²’æœ‰æ‰¾åˆ°è‚¡ç¥¨æ•¸æ“š")
        conn.close()
        return

    processed_list = []
    
    # 2. åˆ†çµ„è¨ˆç®—
    for symbol, group in df.groupby('symbol'):
        group = group.copy().sort_values('date')
        
        # è·³éæ•¸æ“šå¤ªå°‘çš„è‚¡ç¥¨
        if len(group) < 40: 
            continue
        
        # --- ğŸŸ¢ ç¬¬ä¸€æ­¥ï¼šè³‡æ–™æ¸…æ´— ---
        group['daily_change'] = group['close'].pct_change()
        # å¹³æ»‘ç•°å¸¸å€¼ (>60% ä¸”éèˆˆæ«ƒå‰‡è¦–ç‚ºç•°å¸¸)
        is_emerging = group['market'].iloc[0] == 'èˆˆæ«ƒ' if not group['market'].isna().all() else False
        if not is_emerging:
            group.loc[abs(group['daily_change']) > 0.6, 'close'] = np.nan
            group['close'] = group['close'].ffill()
        
        # åŸºç¤æ¬„ä½
        group['prev_close'] = group['close'].shift(1)
        group['avg_vol_20'] = group['volume'].rolling(window=20).mean()
        group['year'] = group['date'].dt.year
        
        # --- ğŸ”´ ç¬¬äºŒæ­¥ï¼šæ¼²åœèˆ‡é•·ç´…å€é–“æ¨™è¨˜ (LU_Type & Brackets) ---
        # æ¼²å¹…ç™¾åˆ†æ¯”
        change_pct = group['daily_change'] * 100
        
        # åˆ¤å®šæ˜¯å¦ç‚ºã€Œå—é™å¸‚å ´æ¼²åœã€
        group['is_limit_up'] = 0
        if not is_emerging:
            # ä¸Šå¸‚æ«ƒ 10% åˆ¤å®š
            group['is_limit_up'] = (group['close'] >= (group['prev_close'] * 1.0945)).astype(int)
        
        # é‡å°ã€Œç„¡é™åˆ¶ã€æˆ–ã€Œé•·ç´…æ£’ã€å®šç¾©å€é–“ (10% - 100%+)
        def label_strength(row):
            chg = row['daily_change'] * 100
            if pd.isna(chg):
                return "NEGATIVE"
            elif chg >= 100: return "RANK_100UP"
            elif chg >= 50: return "RANK_50_100"
            elif chg >= 30: return "RANK_30_50"
            elif chg >= 20: return "RANK_20_30"
            elif chg >= 10: return "RANK_10_20"
            elif chg > 0:   return "POSITIVE"
            return "NEGATIVE"
        
        group['strength_rank'] = group.apply(label_strength, axis=1)

        # æ¼²åœé¡å‹ (LU_Type4)
        conditions = [
            (group['is_limit_up'] == 1) & (group['open'] == group['close']) & (group['high'] == group['low']),
            (group['is_limit_up'] == 1) & (group['open'] > group['prev_close'] * 1.05),
            (group['is_limit_up'] == 1) & (group['volume'] > group['avg_vol_20'] * 2),
            (group['is_limit_up'] == 1)
        ]
        choices = ['NO_VOLUME_LOCK', 'GAP_UP', 'HIGH_VOLUME_LOCK', 'FLOATING']
        group['lu_type'] = np.select(conditions, choices, default=None)

        # é€£æ¿æ¬¡æ•¸
        streak = group['is_limit_up'].groupby((group['is_limit_up'] != group['is_limit_up'].shift()).cumsum()).cumsum()
        group['consecutive_limits'] = np.where(group['is_limit_up'] == 1, streak, 0)

        # --- ğŸŸ£ ç¬¬ä¸‰æ­¥ï¼šå¹´åº¦å·”å³°è²¢ç»åº¦ (ä»¥æœ€é«˜åƒ¹ Peak High è¨ˆç®—) ---
        def calc_peak_contribution(df_year):
            if df_year.empty:
                return df_year
            
            # ç¢ºä¿æœ‰æœ‰æ•ˆçš„é«˜åƒ¹æ•¸æ“š
            valid_high = df_year['high'].dropna()
            if valid_high.empty:
                df_year['peak_date'] = None
                df_year['peak_high_ret'] = np.nan
                df_year['strong_day_contribution'] = np.nan
                return df_year
            
            # æ‰¾åˆ°æœ€é«˜åƒ¹çš„ç´¢å¼•
            peak_idx = valid_high.idxmax()
            if pd.isna(peak_idx):
                df_year['peak_date'] = None
                df_year['peak_high_ret'] = np.nan
                df_year['strong_day_contribution'] = np.nan
                return df_year
            
            # ç²å–å³°å€¼æ—¥æœŸå’Œåƒ¹æ ¼
            peak_date = df_year.loc[peak_idx, 'date'] if peak_idx in df_year.index else None
            peak_price = df_year.loc[peak_idx, 'high'] if peak_idx in df_year.index else np.nan
            
            # ç²å–å¹´åº¦é–‹ç›¤åƒ¹
            if not df_year.empty:
                year_open = df_year.iloc[0]['open']
            else:
                year_open = np.nan
            
            # è¨ˆç®—å¹´åº¦ç¸½å·”å³°å ±é…¬ (å°æ•¸)
            if pd.notna(peak_price) and pd.notna(year_open) and year_open > 0:
                total_peak_log = np.log(peak_price / year_open)
            else:
                total_peak_log = 0
            
            mask_before = df_year['date'] <= peak_date if peak_date else pd.Series(False, index=df_year.index)
            
            # è¨ˆç®—æ‰€æœ‰ã€Œæ¼²å¹… > 10%ã€æ—¥å­çš„ç¸½è²¢ç»
            daily_logs = np.log(df_year['close'] / df_year['prev_close'])
            strong_day_mask = (df_year['daily_change'] >= 0.095) & mask_before
            
            if strong_day_mask.any() and total_peak_log > 0:
                strong_contribution = daily_logs[strong_day_mask].sum()
                strong_day_contribution = (strong_contribution / total_peak_log * 100)
            else:
                strong_day_contribution = 0
            
            df_year['peak_date'] = peak_date
            df_year['peak_high_ret'] = ((peak_price - year_open) / year_open * 100) if pd.notna(peak_price) and pd.notna(year_open) and year_open > 0 else np.nan
            df_year['strong_day_contribution'] = strong_day_contribution
            
            return df_year

        # ä½¿ç”¨ include_groups=False ä¾†é¿å… FutureWarning
        try:
            group = group.groupby('year', group_keys=False, observed=True).apply(
                calc_peak_contribution, include_groups=False
            )
        except Exception as e:
            # å¦‚æœå‡ºéŒ¯ï¼Œä½¿ç”¨èˆŠæ–¹æ³•ä¸¦å¿½ç•¥è­¦å‘Š
            print(f"âš ï¸ è™•ç† {symbol} æ™‚å‡ºç¾è­¦å‘Š: {e}")
            group = group.groupby('year', group_keys=False).apply(calc_peak_contribution)

        # --- ğŸ”µ ç¬¬å››æ­¥ï¼šåŸæœ‰æŠ€è¡“æŒ‡æ¨™ ---
        # MA
        group['ma20'] = group['close'].rolling(window=20).mean()
        group['ma60'] = group['close'].rolling(window=60).mean()
        
        # MACD
        ema12 = group['close'].ewm(span=12, adjust=False).mean()
        ema26 = group['close'].ewm(span=26, adjust=False).mean()
        group['macd'] = ema12 - ema26
        group['macds'] = group['macd'].ewm(span=9, adjust=False).mean()
        group['macdh'] = group['macd'] - group['macds']
        
        # YTD Ret (å¯¦æ¸¬æ”¶ç›¤)
        group['year_start_price'] = group.groupby('year')['close'].transform('first')
        group['ytd_ret'] = ((group['close'] - group['year_start_price']) / group['year_start_price'] * 100).round(2)

        processed_list.append(group)
    
    # 3. æª¢æŸ¥æ˜¯å¦æœ‰è™•ç†å¾Œçš„æ•¸æ“š
    if not processed_list:
        print("âŒ æ²’æœ‰è™•ç†å¾Œçš„æ•¸æ“š")
        conn.close()
        return
    
    # 4. å¯«å›ä¸¦å„ªåŒ–
    df_final = pd.concat(processed_list)
    
    # åˆªé™¤èˆŠè¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    conn.execute("DROP TABLE IF EXISTS stock_analysis")
    
    # å‰µå»ºæ–°è¡¨
    df_final.to_sql('stock_analysis', conn, if_exists='replace', index=False)
    
    # å‰µå»ºç´¢å¼•
    conn.execute("CREATE INDEX IF NOT EXISTS idx_symbol_date ON stock_analysis (symbol, date)")
    
    # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
    total_symbols = df_final['symbol'].nunique()
    date_range = f"{df_final['date'].min()} åˆ° {df_final['date'].max()}"
    
    conn.close()
    
    print(f"""
âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼
ğŸ“Š è™•ç†çµ±è¨ˆï¼š
   - è™•ç†è‚¡ç¥¨æ•¸é‡: {total_symbols}
   - æ•¸æ“šæœŸé–“: {date_range}
   - ç¸½æ•¸æ“šè¡Œæ•¸: {len(df_final)}
   - æ–°å¢ç‰¹å¾µ: æ¼²åœæ¨™è¨˜ã€å¼·åº¦åˆ†ç´šã€å¹´åº¦å·”å³°è²¢ç»åº¦ã€æŠ€è¡“æŒ‡æ¨™ç­‰
    """)

if __name__ == "__main__":
    process_market_data("tw_stock_warehouse.db")
